# ######################################################################
# name: Rust Proxy Scanner (Definitive Edition)
# ######################################################################
name: Rust Proxy Scanner (Definitive Edition)

# Triggers for the workflow
on:
  workflow_dispatch:  # Allows manual triggering from the Actions tab
  schedule:
    - cron: '0 * * * *'  # Runs automatically at the start of every hour

# Ensures only one instance of this workflow runs at a time, canceling older runs.
concurrency:
  group: rust-proxy-scan
  cancel-in-progress: true

# Environment variables available to all jobs
env:
  KEEP_DEPLOYMENTS: '1' # Number of recent Cloudflare deployments to keep
  CACHE_FILE: .cachelastbestip.txt # File to cache the last best IP
  RUST_VERSION: stable

# ==================================================================================
#                                     JOBS
# ==================================================================================
jobs:
  # --------------------------------------------------------------------------------
  # JOB 1: Find the best proxy, update Cloudflare, and trigger deploy
  # --------------------------------------------------------------------------------
  update-proxies:
    runs-on: ubuntu-latest
    timeout-minutes: 20

    # --- Security Best Practice: Define minimum required permissions ---
    permissions:
      contents: write # Needed to commit and push the cache file

    outputs:
      bestip: ${{ steps.scan.outputs.bestip }}

    steps:
    # 1. Checkout the repository code
    - name: ‚¨áÔ∏è Checkout repo
      uses: actions/checkout@v4.1.7

    # 2. Set up the necessary system tools
    - name: üõ†Ô∏è Setup environment
      run: |
        sudo apt-get update -y
        sudo apt-get install -y jq curl netcat-openbsd

    # 3. Cache Rust dependencies for ultra-fast builds
    - name: ‚ö° Cache Cargo dependencies
      uses: actions/cache@v4.0.2
      with:
        path: |
          ~/.cargo/bin/
          ~/.cargo/registry/index/
          ~/.cargo/registry/cache/
          ~/.cargo/git/db/
          target/
        key: ${{ runner.os }}-cargo-${{ hashFiles('**/Cargo.lock') }}

    # 4. Install the Rust compiler
    - name: ü¶Ä Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        toolchain: ${{ env.RUST_VERSION }}

    # 5. Build the scanner program
    #
    # IMPORTANT: The '--locked' flag is a best practice for CI. If this step fails, it means your
    # Cargo.toml and Cargo.lock are out of sync. To fix it, run 'cargo update' locally,
    # then commit and push the updated 'Cargo.lock' file to your repository.
    #
    - name: üì¶ Build Rust project
      run: cargo build --release --locked

    # 6. Run the scanner and find the best IP with the lowest latency
    - name: üì° Run scanner and choose best IP
      id: scan
      run: |
        set -euo pipefail # Exit immediately on any error
        BIN="./target/release/RScanner"
        chmod +x "$BIN"
        : > scan.log # Clear previous log

        echo "üöÄ Starting scanner..."
        # Run scanner and log output, but don't fail the job if the scanner itself exits non-zero
        if ! $BIN 2>&1 | tee -a scan.log; then
          echo "‚ö†Ô∏è WARN: Scanner process exited with an error. Attempting to use partial results or cache."
        fi

        BEST=""
        # Try to find the best IP from the fresh scan log
        if [ -s scan.log ]; then
          # Use awk to parse live proxies and sort by latency (first column)
          BEST=$(awk '
            BEGIN{IGNORECASE=1}
            /PROXY[[:space:]]+LIVE/ {
              if (match($0, /\(([0-9]+)[[:space:]]*ms\)/, lat)) { latency=lat[1] } else next
              if (match($0, /([0-9]{1,3}(\.[0-9]{1,3}){3})/ , addr)) { ip=addr[1] } else next
              print latency, ip
            }
          ' scan.log | sort -n -k1,1 | head -n 1 | awk '{print $2}')
        fi

        if [ -n "$BEST" ]; then
          echo "‚úÖ Selected candidate from fresh scan: $BEST"
        else
          echo "‚ÑπÔ∏è No live candidates found in scan log. Checking cache..."
          if [ -f "${CACHE_FILE}" ]; then
            BEST="$(tr -d ' \r\n' < ${CACHE_FILE})"
            echo "‚Ü™Ô∏è Using cached BEST IP: $BEST"
          fi
        fi

        # If still no IP, fail the workflow
        if [ -z "${BEST:-}" ]; then
          echo "‚ùå ERROR: No best candidate found from scan or cache. Aborting."
          exit 1
        fi

        echo "üèÜ Final selected IP: $BEST"
        echo "$BEST" > ${CACHE_FILE}
        echo "bestip=$BEST" >> "$GITHUB_OUTPUT"

    # 7. Upload the scan log as an artifact for debugging
    - name: üìú Upload scan.log for debug
      uses: actions/upload-artifact@v4.3.4
      with:
        name: scan-log-${{ github.run_id }}
        path: scan.log

    # 8. Commit and push the cached IP file to the repository
    - name: üíæ Commit & push cache
      run: |
        git config --global user.name "GitHub Actions Bot"
        git config --global user.email "actions@github.com"
        git add ${{ env.CACHE_FILE }}
        # Only commit and push if there are actual changes
        if ! git diff --cached --quiet; then
          git commit -m "chore(bot): Update proxy IP to ${{ steps.scan.outputs.bestip }}"
          git push
        else
          echo "‚úÖ No change in best IP. Nothing to commit."
        fi

    # 9. Update the environment variable in Cloudflare Pages with retry logic
    - name: ‚òÅÔ∏è Update Cloudflare Pages env var
      env:
        CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CF_PROJECT_NAME: ${{ secrets.CF_PROJECT_NAME }}
        CF_VAR_NAME: ${{ secrets.CF_VAR_NAME }}
        BESTIP: ${{ steps.scan.outputs.bestip }}
      run: |
        set -euo pipefail
        API_URL="https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/pages/projects/${CF_PROJECT_NAME}"
        JSON_PAYLOAD=$(jq -n \
          --arg var_name "$CF_VAR_NAME" \
          --arg var_value "$BESTIP" \
          '{deployment_configs: {production: {env_vars: {($var_name): {value: $var_value}}}}}')

        echo "Attempting to update Cloudflare variable..."
        for i in {1..3}; do
          echo "Attempt $i of 3..."
          response_code=$(curl -s --fail -w "%{http_code}" -o /dev/null -X PATCH "$API_URL" \
            -H "Authorization: Bearer ${CF_API_TOKEN}" \
            -H "Content-Type: application/json" \
            --data "$JSON_PAYLOAD")

          if [ "$response_code" -eq 200 ]; then
            echo "‚úÖ Cloudflare variable updated successfully!"
            exit 0
          fi
          echo "‚ö†Ô∏è Attempt $i failed with HTTP status $response_code. Retrying in 5 seconds..."
          sleep 5
        done
        echo "‚ùå ERROR: Failed to update Cloudflare variable after 3 attempts."
        exit 1

    # 10. Trigger a new deployment in Cloudflare Pages
    - name: üöÄ Trigger Cloudflare Pages deploy
      if: success()
      env:
        CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CF_PROJECT_NAME: ${{ secrets.CF_PROJECT_NAME }}
      run: |
        set -euo pipefail
        API_URL="https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/pages/projects/${CF_PROJECT_NAME}/deployments"
        echo "Attempting to trigger a new deployment..."
        # Similar retry logic for triggering deployment
        for i in {1..3}; do
          echo "Attempt $i of 3..."
           response_code=$(curl -s --fail -w "%{http_code}" -o /dev/null -X POST "$API_URL" \
            -H "Authorization: Bearer ${CF_API_TOKEN}" \
            -H "Content-Type: application/json")

          if [ "$response_code" -eq 200 ]; then
            echo "‚úÖ New deployment triggered successfully."
            exit 0
          fi
          echo "‚ö†Ô∏è Attempt $i failed with HTTP status $response_code. Retrying in 5 seconds..."
          sleep 5
        done
        echo "‚ùå ERROR: Failed to trigger Cloudflare deployment after 3 attempts."
        exit 1

    # 11. Clean up old deployments to avoid hitting Cloudflare limits
    - name: üßπ Cleanup old Cloudflare deployments
      if: always() # This step runs even if previous steps fail
      env:
        CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CF_PROJECT_NAME: ${{ secrets.CF_PROJECT_NAME }}
      run: |
        set -euo pipefail
        echo "Fetching list of deployments to clean up..."
        DEPLOYMENTS=$(curl -s --fail -X GET "https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/pages/projects/${CF_PROJECT_NAME}/deployments" -H "Authorization: Bearer ${CF_API_TOKEN}")
        IDS_TO_DELETE=$(echo "$DEPLOYMENTS" | jq -r ".result | sort_by(.created_on) | reverse | .[${KEEP_DEPLOYMENTS}:] | .[] | .id")

        if [ -z "$IDS_TO_DELETE" ]; then
            echo "‚úÖ No old deployments to delete."
            exit 0
        fi
        for id in $IDS_TO_DELETE; do
            echo "Deleting old deployment: $id"
            curl -s --fail -X DELETE "https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/pages/projects/${CF_PROJECT_NAME}/deployments/${id}" -H "Authorization: Bearer ${CF_API_TOKEN}" > /dev/null
        done
        echo "‚úÖ Cloudflare cleanup complete."

  # --------------------------------------------------------------------------------
  # JOB 2: Cleanup workflow runs to keep the Actions history clean
  # --------------------------------------------------------------------------------
  cleanup-runs:
    runs-on: ubuntu-latest
    needs: update-proxies # Runs only after the main job is finished
    if: always() # Runs regardless of whether the main job succeeded or failed

    # --- Security Best Practice: This job needs permission to delete workflow runs ---
    permissions:
      actions: write

    steps:
      - name: üóëÔ∏è Delete old workflow runs
        uses: Mattraks/delete-workflow-runs@v2.0.0
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 7       # Keeps workflow runs for 7 days
          keep_minimum_runs: 5 # Always keeps the 5 most recent runs for debugging
