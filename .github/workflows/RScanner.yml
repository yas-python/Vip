# Workflow name
name: Rust Proxy Scanner for Cloudflare Workers (Ultimate)

# Workflow triggers
on:
  workflow_dispatch:  # Manual trigger
  schedule:
    - cron: '0 * * * *'  # Automatic execution every hour

# Ensure only one instance runs at a time
concurrency:
  group: rust-proxy-worker-scan
  cancel-in-progress: true

# Global environment variables
env:
  CARGO_TERM_COLOR: always
  KEEP_DEPLOYMENTS: '3'  # Number of recent Worker deployments to retain
  CACHE_FILE: .cachelastbestip.txt  # Cache file for the best IP address
  RUST_CACHE_KEY: v1  # Increment to invalidate Rust cache manually
  NODE_VERSION: '18'  # Node.js version for Wrangler CLI

jobs:
  # Main job: Scan proxies and update Cloudflare Worker
  update-worker-proxy:
    runs-on: ubuntu-latest
    timeout-minutes: 35
    outputs:
      bestip: ${{ steps.scan.outputs.bestip }}
      deployment_id: ${{ steps.deploy.outputs.deployment_id }}

    steps:
    # Step 1: Checkout repository code
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        fetch-depth: 0  # Full history for better Git operations

    # Step 2: Setup required system tools
    - name: Setup system environment
      run: |
        sudo apt-get update -y
        sudo apt-get install -y jq curl python3 python3-pip netcat-openbsd
        python3 -m pip install --no-cache-dir requests
        echo "‚úÖ System environment configured successfully"

    # Step 3: Setup Node.js for Wrangler CLI
    - name: Setup Node.js
      uses: actions/setup-node@v4
      with:
        node-version: ${{ env.NODE_VERSION }}
        cache: 'npm'

    # Step 4: Install Wrangler CLI (Cloudflare Workers deployment tool)
    - name: Install Wrangler CLI
      run: |
        npm install -g wrangler
        wrangler --version
        echo "‚úÖ Wrangler CLI installed successfully"

    # Step 5: Restore cached best IP from previous runs
    - name: Restore best IP cache
      uses: actions/cache@v4
      id: ip-cache
      with:
        path: ${{ env.CACHE_FILE }}
        key: rust-proxy-worker-best-ip-v2-${{ github.run_id }}
        restore-keys: |
          rust-proxy-worker-best-ip-v2-

    # Step 6: Install Rust toolchain
    - name: Install Rust toolchain
      uses: dtolnay/rust-toolchain@stable
      with:
        components: rustfmt, clippy

    # Step 7: Cache Rust dependencies for faster builds
    # Note: "Cache not found" on first run is normal - cache will be saved at job end
    - name: Cache Rust dependencies
      uses: Swatinem/rust-cache@v2
      with:
        key: ${{ env.RUST_CACHE_KEY }}
        cache-on-failure: true

    # Step 8: Build the Rust scanner binary
    - name: Build Rust scanner project
      run: |
        echo "üî® Building Rust project in release mode..."
        cargo build --release --verbose
        echo "‚úÖ Build completed successfully"

    # Step 9: Execute scanner and identify the best proxy IP
    - name: Execute proxy scanner and select optimal IP
      id: scan
      run: |
        set -euo pipefail  # Strict error handling
        
        BIN="./target/release/RScanner"
        chmod +x "$BIN"
        : > scan.log  # Initialize empty log file
        
        echo "üöÄ Initiating comprehensive proxy scan..."
        echo "‚è∞ Scan started at: $(date -u +"%Y-%m-%d %H:%M:%S UTC")"
        
        # Run scanner and capture output (continue even if scanner exits with error)
        $BIN 2>&1 | tee -a scan.log || {
          echo "‚ö†Ô∏è Scanner process terminated with non-zero exit code"
          echo "   Will attempt to extract results from partial scan data..."
        }
        
        BEST=""
        BEST_LAT=""
        
        echo ""
        echo "üîç Analyzing scan results for optimal proxy candidate..."
        
        # Parse scan log to extract IP addresses and latencies
        # This AWK script looks for lines indicating live proxies with latency measurements
        awk '
          BEGIN { IGNORECASE=1 }
          /PROXY[[:space:]]+LIVE/ {
            # Extract latency in milliseconds
            if (match($0, /\(([0-9]+)[[:space:]]*ms\)/, lat)) { 
              latency = lat[1] 
            } else { 
              next 
            }
            
            # Extract IP address
            if (match($0, /([0-9]{1,3}(\.[0-9]{1,3}){3})/, addr)) { 
              ip = addr[1] 
            } else { 
              next 
            }
            
            # Output: latency ip_address
            print latency, ip
          }
        ' scan.log | sort -n -k1,1 > candidates.txt
        
        # Check if we found any valid candidates
        if [ -s candidates.txt ]; then
          CANDIDATE_COUNT=$(wc -l < candidates.txt)
          echo "‚úÖ Found ${CANDIDATE_COUNT} live proxy candidate(s)"
          
          # Select the best (lowest latency) candidate
          BEST_LAT=$(awk 'NR==1 {print $1}' candidates.txt)
          BEST=$(awk 'NR==1 {print $2}' candidates.txt)
          
          echo "üèÜ Selected optimal proxy from current scan:"
          echo "   IP Address: $BEST"
          echo "   Latency: ${BEST_LAT} ms"
          
          # Display top 5 candidates for reference
          echo ""
          echo "üìä Top 5 candidates (latency, IP):"
          head -n 5 candidates.txt | awk '{print "   " $1 " ms - " $2}'
        else
          echo "üü° No live proxies discovered in current scan"
          echo "   Attempting to recover from cache..."
        fi
        
        # Fallback to cached IP if no new candidate found
        if [ -z "$BEST" ] && [ -f "${CACHE_FILE}" ]; then
          BEST="$(tr -d ' \r\n\t' < ${CACHE_FILE})"
          if [ -n "$BEST" ]; then
            echo "‚úÖ Successfully recovered cached IP: $BEST"
            echo "   Note: Using previous best result due to scan failure"
          fi
        fi
        
        # Validate that we have a usable IP address
        if [ -z "${BEST:-}" ]; then
          echo ""
          echo "‚ùå CRITICAL ERROR: No valid proxy IP found"
          echo "   - Current scan yielded no results"
          echo "   - No cached IP available for fallback"
          echo "   - Cannot proceed with Worker deployment"
          exit 1
        fi
        
        # Final validation: check IP format
        if ! echo "$BEST" | grep -qE '^([0-9]{1,3}\.){3}[0-9]{1,3}$'; then
          echo "‚ùå ERROR: Invalid IP address format: $BEST"
          exit 1
        fi
        
        echo ""
        echo "üíæ Persisting selected IP to cache and workflow output..."
        echo "$BEST" > "${CACHE_FILE}"
        echo "bestip=$BEST" >> "$GITHUB_OUTPUT"
        echo ""
        echo "‚úÖ Scan phase completed successfully"
        echo "üìç Best IP: $BEST"

    # Step 10: Upload scan log as artifact for debugging and analysis
    - name: Upload scan log artifact
      if: always()  # Run even if previous steps failed
      uses: actions/upload-artifact@v4
      with:
        name: proxy-scan-log-${{ github.run_id }}
        path: |
          scan.log
          candidates.txt
        retention-days: 7  # Keep logs for 7 days

    # Step 11: Update Worker environment variable via Cloudflare API
    - name: Update Cloudflare Worker environment variable
      env:
        CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CF_WORKER_NAME: ${{ secrets.CF_WORKER_NAME }}
        CF_VAR_NAME: ${{ secrets.CF_VAR_NAME }}
        BESTIP: ${{ steps.scan.outputs.bestip }}
      run: |
        set -euo pipefail
        
        echo "üîß Preparing to update Cloudflare Worker environment variable..."
        
        # Validate required inputs
        if [ -z "${BESTIP:-}" ]; then
          echo "‚ùå ERROR: BESTIP variable is empty - cannot update Worker"
          exit 1
        fi
        
        if [ -z "${CF_WORKER_NAME:-}" ]; then
          echo "‚ùå ERROR: CF_WORKER_NAME secret not configured"
          exit 1
        fi
        
        echo "üìã Configuration:"
        echo "   Worker Name: ${CF_WORKER_NAME}"
        echo "   Variable Name: ${CF_VAR_NAME}"
        echo "   New Value: ${BESTIP}"
        
        # Cloudflare API endpoint for Worker settings
        API_URL="https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/workers/scripts/${CF_WORKER_NAME}/settings"
        
        # First, fetch current environment variables to preserve them
        echo ""
        echo "üì• Fetching current Worker configuration..."
        CURRENT_CONFIG=$(curl -s --fail -X GET "$API_URL" \
          -H "Authorization: Bearer ${CF_API_TOKEN}" \
          -H "Content-Type: application/json")
        
        # Extract existing bindings (preserve KV, Durable Objects, etc.)
        EXISTING_BINDINGS=$(echo "$CURRENT_CONFIG" | jq -r '.result.bindings // []')
        
        # Create updated environment variables JSON
        # This preserves all existing env vars and updates/adds our proxy IP variable
        UPDATED_ENV_VARS=$(echo "$CURRENT_CONFIG" | jq -r '.result.bindings // []' | \
          jq --arg var_name "$CF_VAR_NAME" --arg var_value "$BESTIP" '
            map(select(.name != $var_name)) + 
            [{
              "type": "plain_text",
              "name": $var_name,
              "text": $var_value
            }]
          ')
        
        # Construct the complete settings payload
        JSON_PAYLOAD=$(jq -n \
          --argjson bindings "$UPDATED_ENV_VARS" \
          '{bindings: $bindings}')
        
        echo "üì° Sending PATCH request to Cloudflare API..."
        RESPONSE=$(curl -s -w "\n%{http_code}" -X PATCH "$API_URL" \
          -H "Authorization: Bearer ${CF_API_TOKEN}" \
          -H "Content-Type: application/json" \
          --data "$JSON_PAYLOAD")
        
        HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
        BODY=$(echo "$RESPONSE" | sed '$d')
        
        # Check if request was successful
        if [ "$HTTP_CODE" -ne 200 ]; then
          echo "‚ùå Error: Cloudflare API returned HTTP status $HTTP_CODE"
          echo "Response details:"
          echo "$BODY" | jq '.' || echo "$BODY"
          exit 1
        fi
        
        # Verify the update was successful
        SUCCESS=$(echo "$BODY" | jq -r '.success')
        if [ "$SUCCESS" != "true" ]; then
          echo "‚ùå Error: API request completed but reported failure"
          echo "$BODY" | jq '.'
          exit 1
        fi
        
        echo "‚úÖ Worker environment variable '${CF_VAR_NAME}' updated successfully"
        echo "   New value: ${BESTIP}"

    # Step 12: Deploy updated Worker to Cloudflare
    - name: Deploy Cloudflare Worker
      id: deploy
      env:
        CLOUDFLARE_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CLOUDFLARE_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_WORKER_NAME: ${{ secrets.CF_WORKER_NAME }}
      run: |
        set -euo pipefail
        
        echo "üöÄ Initiating Cloudflare Worker deployment..."
        
        # Check if wrangler.toml exists; if not, create a minimal one
        if [ ! -f "wrangler.toml" ]; then
          echo "üìù Creating minimal wrangler.toml configuration..."
          cat > wrangler.toml << EOF
        name = "${CF_WORKER_NAME}"
        main = "src/index.js"
        compatibility_date = "$(date +%Y-%m-%d)"
        
        [env.production]
        vars = { ${CF_VAR_NAME} = "${BESTIP}" }
        EOF
          echo "‚úÖ Configuration file created"
        fi
        
        # Verify Worker source code exists
        if [ ! -f "src/index.js" ] && [ ! -f "worker.js" ] && [ ! -f "index.js" ]; then
          echo "‚ö†Ô∏è Warning: No Worker script found (src/index.js, worker.js, or index.js)"
          echo "   Creating a minimal Worker script for testing..."
          mkdir -p src
          cat > src/index.js << 'EOF'
        export default {
          async fetch(request, env) {
            return new Response(JSON.stringify({
              PROXYIP: env.PROXYIP || 'not set',
              timestamp: new Date().toISOString(),
              status: 'active'
            }), {
              headers: { 'Content-Type': 'application/json' }
            });
          }
        };
        EOF
        fi
        
        echo "üì¶ Deploying Worker to Cloudflare..."
        
        # Deploy using Wrangler CLI
        DEPLOY_OUTPUT=$(wrangler deploy --env production 2>&1) || {
          echo "‚ùå Deployment failed"
          echo "$DEPLOY_OUTPUT"
          exit 1
        }
        
        echo "$DEPLOY_OUTPUT"
        
        # Extract deployment ID if available
        DEPLOYMENT_ID=$(echo "$DEPLOY_OUTPUT" | grep -oP 'deployment-id: \K[a-f0-9-]+' || echo "unknown")
        echo "deployment_id=$DEPLOYMENT_ID" >> "$GITHUB_OUTPUT"
        
        echo ""
        echo "‚úÖ Worker deployed successfully"
        echo "üÜî Deployment ID: $DEPLOYMENT_ID"
        echo "üåê Worker URL: https://${CF_WORKER_NAME}.workers.dev"

    # Step 13: Verify Worker deployment and functionality
    - name: Verify Worker deployment
      env:
        CF_WORKER_NAME: ${{ secrets.CF_WORKER_NAME }}
        BESTIP: ${{ steps.scan.outputs.bestip }}
      run: |
        set -euo pipefail
        
        echo "üîç Verifying Worker deployment and configuration..."
        
        WORKER_URL="https://${CF_WORKER_NAME}.workers.dev"
        
        # Wait a moment for deployment to propagate
        echo "‚è≥ Waiting 10 seconds for deployment propagation..."
        sleep 10
        
        # Test Worker endpoint
        echo "üì° Testing Worker endpoint: $WORKER_URL"
        RESPONSE=$(curl -s -w "\n%{http_code}" --max-time 15 "$WORKER_URL") || {
          echo "‚ö†Ô∏è Warning: Could not reach Worker endpoint (this may be normal if Worker has custom routing)"
          exit 0
        }
        
        HTTP_CODE=$(echo "$RESPONSE" | tail -n1)
        BODY=$(echo "$RESPONSE" | sed '$d')
        
        echo "Response Status: $HTTP_CODE"
        
        if [ "$HTTP_CODE" -eq 200 ]; then
          echo "‚úÖ Worker is responding successfully"
          echo "Response:"
          echo "$BODY" | jq '.' 2>/dev/null || echo "$BODY"
          
          # If response is JSON, verify proxy IP is set correctly
          if echo "$BODY" | jq -e '.PROXYIP' >/dev/null 2>&1; then
            PROXYIP=$(echo "$BODY" | jq -r '.PROXYIP')
            if [ "$PROXYIP" = "$BESTIP" ]; then
              echo "‚úÖ Proxy IP verified correctly in Worker: $PROXYIP"
            else
              echo "‚ö†Ô∏è Warning: Proxy IP mismatch (expected: $BESTIP, got: $PROXYIP)"
            fi
          fi
        else
          echo "‚ö†Ô∏è Unexpected HTTP status: $HTTP_CODE"
        fi

    # Step 14: Cleanup old Worker versions/deployments
    - name: Cleanup old Worker deployments
      if: always()
      env:
        CF_ACCOUNT_ID: ${{ secrets.CF_ACCOUNT_ID }}
        CF_API_TOKEN: ${{ secrets.CF_API_TOKEN }}
        CF_WORKER_NAME: ${{ secrets.CF_WORKER_NAME }}
        KEEP: ${{ env.KEEP_DEPLOYMENTS }}
      run: |
        set -euo pipefail
        
        echo "üßπ Starting cleanup of old Worker deployments..."
        
        # Cloudflare Workers API endpoint for deployments
        API_URL="https://api.cloudflare.com/client/v4/accounts/${CF_ACCOUNT_ID}/workers/scripts/${CF_WORKER_NAME}/deployments"
        
        echo "üì• Fetching deployment history..."
        DEPLOYMENTS_RESPONSE=$(curl -s --fail -X GET "$API_URL" \
          -H "Authorization: Bearer ${CF_API_TOKEN}" \
          -H "Content-Type: application/json") || {
          echo "‚ö†Ô∏è Could not fetch deployments (API may not support this endpoint)"
          echo "   Skipping cleanup step"
          exit 0
        }
        
        # Check if we got valid JSON response
        if ! echo "$DEPLOYMENTS_RESPONSE" | jq -e '.result' >/dev/null 2>&1; then
          echo "‚ö†Ô∏è No deployment history available or API structure changed"
          echo "   Skipping cleanup"
          exit 0
        fi
        
        # Extract deployment IDs to delete (keep most recent N deployments)
        IDS_TO_DELETE=$(echo "$DEPLOYMENTS_RESPONSE" | jq -r \
          ".result | sort_by(.created_on) | reverse | .[${KEEP}:] | .[]? | .id" 2>/dev/null || echo "")
        
        if [ -z "$IDS_TO_DELETE" ]; then
          echo "üëç No old deployments to clean up"
          echo "   Current deployment count is within retention limit"
          exit 0
        fi
        
        DELETION_COUNT=$(echo "$IDS_TO_DELETE" | wc -l)
        echo "üóëÔ∏è  Found ${DELETION_COUNT} old deployment(s) to remove"
        echo ""
        
        # Delete each old deployment
        for deployment_id in $IDS_TO_DELETE; do
          echo "   Deleting deployment: $deployment_id"
          
          DELETE_RESPONSE=$(curl -s -w "\n%{http_code}" -X DELETE \
            "${API_URL}/${deployment_id}" \
            -H "Authorization: Bearer ${CF_API_TOKEN}") || {
            echo "     ‚ö†Ô∏è Delete request failed for $deployment_id"
            continue
          }
          
          DELETE_HTTP_CODE=$(echo "$DELETE_RESPONSE" | tail -n1)
          DELETE_BODY=$(echo "$DELETE_RESPONSE" | sed '$d')
          
          if [ "$DELETE_HTTP_CODE" -eq 200 ] || [ "$DELETE_HTTP_CODE" -eq 204 ]; then
            echo "     ‚úÖ Successfully deleted"
          else
            echo "     ‚ö†Ô∏è Failed to delete (HTTP $DELETE_HTTP_CODE)"
            echo "     Response: $DELETE_BODY"
          fi
        done
        
        echo ""
        echo "‚úÖ Deployment cleanup completed"

  # Job 2: Cleanup old workflow runs to maintain clean Actions history
  cleanup-workflow-runs:
    runs-on: ubuntu-latest
    needs: update-worker-proxy
    if: always()  # Run even if main job fails
    permissions:
      actions: write  # Required permission to delete workflow runs
    
    steps:
      - name: Delete old workflow runs
        uses: Mattraks/delete-workflow-runs@v2
        with:
          token: ${{ github.token }}
          repository: ${{ github.repository }}
          retain_days: 0  # Delete all runs older than today
          keep_minimum_runs: 3  # But always keep at least 3 most recent runs
          delete_workflow_pattern: 'Rust Proxy Scanner for Cloudflare Workers (Ultimate)'
        continue-on-error: true  # Don't fail workflow if cleanup fails

      - name: Cleanup summary
        if: always()
        run: |
          echo "‚úÖ Workflow run cleanup completed"
          echo "   Retention policy: Keep minimum 3 most recent runs"
          echo "   This helps maintain a clean Actions history and stay within GitHub limits"
